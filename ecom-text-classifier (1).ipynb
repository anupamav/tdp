{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libararies","metadata":{}},{"cell_type":"code","source":"import io\nimport csv\nimport random\nfrom sklearn.metrics import *\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:06:59.465359Z","iopub.execute_input":"2023-05-26T10:06:59.465783Z","iopub.status.idle":"2023-05-26T10:06:59.794735Z","shell.execute_reply.started":"2023-05-26T10:06:59.465750Z","shell.execute_reply":"2023-05-26T10:06:59.793476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining useful global variables\n* NUM_WORDS: The maximum number of words to keep, based on word frequency.\n* EMBEDDING_DIM: Dimension of the dense embedding, will be used in the embedding layer of the model. \n* MAXLEN: Maximum length of all sequences. \n* PADDING: Padding strategy (pad either before or after each sequence.). Defaults to 'post'.\n* OOV_TOKEN: Token to replace out-of-vocabulary words during text_to_sequence calls. Defaults to \"<OOV>\".\n* TRAINING_SPLIT: Proportion of data used for training. ","metadata":{}},{"cell_type":"code","source":"NUM_WORDS = 10000\nEMBEDDING_DIM = 160\nMAXLEN = 1200\nPADDING = 'post'\nOOV_TOKEN = \"<OOV>\"\nTRAINING_SPLIT = 0.95","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:06:59.796650Z","iopub.execute_input":"2023-05-26T10:06:59.796991Z","iopub.status.idle":"2023-05-26T10:06:59.801199Z","shell.execute_reply.started":"2023-05-26T10:06:59.796964Z","shell.execute_reply":"2023-05-26T10:06:59.800497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading and pre-processing the data\nFunctions to remove stopwords from text and to load the data from a csv file.","metadata":{}},{"cell_type":"code","source":"def remove_stopwords(sentence):\n    \"\"\"\n    Removes a list of stopwords\n    \n    Args:\n        sentence (string): sentence to remove the stopwords from\n    \n    Returns:\n        sentence (string): lowercase sentence without the stopwords\n    \"\"\"\n    # List of stopwords\n    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n    \n    # Sentence converted to lowercase-only\n    sentence = sentence.lower()\n\n    words = sentence.split()\n    no_words = [w for w in words if w not in stopwords]\n    sentence = \" \".join(no_words)\n\n    return sentence","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:06:59.802311Z","iopub.execute_input":"2023-05-26T10:06:59.802798Z","iopub.status.idle":"2023-05-26T10:06:59.820501Z","shell.execute_reply.started":"2023-05-26T10:06:59.802771Z","shell.execute_reply":"2023-05-26T10:06:59.819383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_data_from_file(filename):\n    \"\"\"\n    Extracts sentences and labels from a CSV file\n    \n    Args:\n        filename (string): path to the CSV file\n    \n    Returns:\n        sentences, labels (list of string, list of string): tuple containing lists of sentences and labels\n    \"\"\"\n    sentences = []\n    labels = []\n    with open(filename, 'r') as csvfile:\n        reader = csv.reader(csvfile, delimiter=',')\n        next(reader)\n        for row in reader:\n            if row[0] =='Clothing & Accessories':\n                row[0] = 'ClothingAccessories'\n            labels.append(row[0])\n            sentence = row[1]\n            sentence = remove_stopwords(sentence)\n            sentences.append(sentence)\n\n    return sentences, labels","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:06:59.823565Z","iopub.execute_input":"2023-05-26T10:06:59.824093Z","iopub.status.idle":"2023-05-26T10:06:59.834305Z","shell.execute_reply.started":"2023-05-26T10:06:59.824054Z","shell.execute_reply":"2023-05-26T10:06:59.833016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences, labels = parse_data_from_file(\"/kaggle/input/ecommerce-text-classification/ecommerceDataset.csv\")\nprint(f\"There are {len(sentences)} sentences in the dataset.\\n\")\nprint(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\nprint(f\"There are {len(labels)} labels in the dataset.\\n\")\nprint(f\"The first 5 labels are {labels[:5]}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:06:59.835985Z","iopub.execute_input":"2023-05-26T10:06:59.836426Z","iopub.status.idle":"2023-05-26T10:07:12.216783Z","shell.execute_reply.started":"2023-05-26T10:06:59.836385Z","shell.execute_reply":"2023-05-26T10:07:12.215810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Shuffle two lists with same order\nlst = list(zip(sentences, labels))\nrandom.shuffle(lst)\nsentences, labels = zip(*lst)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:07:12.218143Z","iopub.execute_input":"2023-05-26T10:07:12.218499Z","iopub.status.idle":"2023-05-26T10:07:12.336688Z","shell.execute_reply.started":"2023-05-26T10:07:12.218447Z","shell.execute_reply":"2023-05-26T10:07:12.335514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\nprint(f\"There are {len(labels)} labels in the dataset.\\n\")\nprint(f\"The first 5 labels are {labels[:5]}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:07:12.337877Z","iopub.execute_input":"2023-05-26T10:07:12.338228Z","iopub.status.idle":"2023-05-26T10:07:12.346601Z","shell.execute_reply.started":"2023-05-26T10:07:12.338197Z","shell.execute_reply":"2023-05-26T10:07:12.345337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training - Validation Split\n\nGiven the training split size, `train_val_split()` function splits the full lists of sentences and labels into training and validation sentences and labels.","metadata":{}},{"cell_type":"code","source":"def train_val_split(sentences, labels, training_split):\n    \"\"\"\n    Splits the dataset into training and validation sets\n    \n    Args:\n        sentences (list of string): lower-cased sentences without stopwords\n        labels (list of string): list of labels\n        training split (float): proportion of the dataset to convert to include in the train set\n    \n    Returns:\n        train_sentences, validation_sentences, train_labels, validation_labels - lists containing the data splits\n    \"\"\"\n    \n    # Compute the number of sentences that will be used for training (should be an integer)\n    train_size = int(training_split * len(sentences))\n\n    # Split the sentences and labels into train/validation splits\n    train_sentences = sentences[0:train_size]\n    train_labels = labels[0:train_size]\n\n    validation_sentences = sentences[train_size:]\n    validation_labels = labels[train_size:]\n    \n    \n    return train_sentences, validation_sentences, train_labels, validation_labels","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:07:12.347907Z","iopub.execute_input":"2023-05-26T10:07:12.348227Z","iopub.status.idle":"2023-05-26T10:07:12.362365Z","shell.execute_reply.started":"2023-05-26T10:07:12.348200Z","shell.execute_reply":"2023-05-26T10:07:12.361114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sentences, val_sentences, train_labels, val_labels = train_val_split(sentences, labels, TRAINING_SPLIT)\nprint(f\"There are {len(train_sentences)} sentences for training.\\n\")\nprint(f\"There are {len(train_labels)} labels for training.\\n\")\nprint(f\"There are {len(val_sentences)} sentences for validation.\\n\")\nprint(f\"There are {len(val_labels)} labels for validation.\")","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:07:12.364227Z","iopub.execute_input":"2023-05-26T10:07:12.364652Z","iopub.status.idle":"2023-05-26T10:07:12.391217Z","shell.execute_reply.started":"2023-05-26T10:07:12.364608Z","shell.execute_reply":"2023-05-26T10:07:12.389865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization - Sequences and padding\n`fit_tokenizer` function returns a Tokenizer","metadata":{}},{"cell_type":"code","source":"def fit_tokenizer(train_sentences, num_words, oov_token):\n    \"\"\"\n    Instantiates the Tokenizer class on the training sentences\n    \n    Args:\n        train_sentences (list of string): lower-cased sentences without stopwords to be used for training\n        num_words (int) - number of words to keep when tokenizing\n        oov_token (string) - symbol for the out-of-vocabulary token\n    \n    Returns:\n        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n    \"\"\"\n\n    # Instantiate the Tokenizer class, passing in the correct values for num_words and oov_token\n    tokenizer = Tokenizer(num_words = num_words, oov_token=oov_token)\n    \n    # Fit the tokenizer to the training sentences\n    tokenizer.fit_on_texts(train_sentences)\n       \n    return tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:07:12.396038Z","iopub.execute_input":"2023-05-26T10:07:12.396383Z","iopub.status.idle":"2023-05-26T10:07:12.405962Z","shell.execute_reply.started":"2023-05-26T10:07:12.396356Z","shell.execute_reply":"2023-05-26T10:07:12.404783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = fit_tokenizer(train_sentences, NUM_WORDS, OOV_TOKEN)\nword_index = tokenizer.word_index\n\nprint(f\"Vocabulary contains {len(word_index)} words\\n\")\nprint(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:07:12.407712Z","iopub.execute_input":"2023-05-26T10:07:12.408033Z","iopub.status.idle":"2023-05-26T10:07:18.374801Z","shell.execute_reply.started":"2023-05-26T10:07:12.408006Z","shell.execute_reply":"2023-05-26T10:07:18.373376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`seq_and_pad` function will convert each text data point into its padded sequence representation","metadata":{}},{"cell_type":"code","source":"def seq_and_pad(sentences, tokenizer, padding, maxlen):\n    \"\"\"\n    Generates an array of token sequences and pads them to the same length\n    \n    Args:\n        sentences (list of string): list of sentences to tokenize and pad\n        tokenizer (object): Tokenizer instance containing the word-index dictionary\n        padding (string): type of padding to use\n        maxlen (int): maximum length of the token sequence\n    \n    Returns:\n        padded_sequences (array of int): tokenized sentences padded to the same length\n    \"\"\"    \n       \n   # Convert sentences to sequences\n    sequences = tokenizer.texts_to_sequences(sentences)\n    \n    # Pad the sequences using the correct padding and maxlen\n    padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding=padding)\n\n    \n    return padded_sequences","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:07:18.376214Z","iopub.execute_input":"2023-05-26T10:07:18.376583Z","iopub.status.idle":"2023-05-26T10:07:18.382945Z","shell.execute_reply.started":"2023-05-26T10:07:18.376552Z","shell.execute_reply":"2023-05-26T10:07:18.381800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_padded_seq = seq_and_pad(train_sentences, tokenizer, PADDING, MAXLEN)\nval_padded_seq = seq_and_pad(val_sentences, tokenizer, PADDING, MAXLEN)\n\nprint(f\"Padded training sequences have shape: {train_padded_seq.shape}\\n\")\nprint(f\"Padded validation sequences have shape: {val_padded_seq.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:07:18.384942Z","iopub.execute_input":"2023-05-26T10:07:18.385310Z","iopub.status.idle":"2023-05-26T10:07:23.820442Z","shell.execute_reply.started":"2023-05-26T10:07:18.385282Z","shell.execute_reply":"2023-05-26T10:07:23.819226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`tokenize_labels` function tokenizes the labels","metadata":{}},{"cell_type":"code","source":"def tokenize_labels(all_labels, split_labels):\n    \"\"\"\n    Tokenizes the labels\n    \n    Args:\n        all_labels (list of string): labels to generate the word-index from\n        split_labels (list of string): labels to tokenize\n    \n    Returns:\n        label_seq_np (array of int): tokenized labels\n    \"\"\"\n    # Instantiate the Tokenizer (no additional arguments needed)\n    label_tokenizer = Tokenizer()\n    \n    # Fit the tokenizer on all the labels\n    \n    label_tokenizer.fit_on_texts(all_labels)\n    \n    # Convert labels to sequences\n    label_seq = label_tokenizer.texts_to_sequences(split_labels)\n    \n    # Convert sequences to a numpy array. Substact 1 from every entry in the array\n    label_seq_np = np.array(label_seq)-1\n\n    return label_seq_np","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:07:23.821840Z","iopub.execute_input":"2023-05-26T10:07:23.822635Z","iopub.status.idle":"2023-05-26T10:07:23.828710Z","shell.execute_reply.started":"2023-05-26T10:07:23.822603Z","shell.execute_reply":"2023-05-26T10:07:23.827554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_label_seq = tokenize_labels(labels, val_labels)\ntrain_label_seq = tokenize_labels(labels, train_labels)\nprint(f\"First 5 labels of the training set should look like this:\\n{train_label_seq[:8]}\\n\")\nprint(f\"First 5 labels of the validation set should look like this:\\n{val_label_seq[:5]}\\n\")\nprint(f\"Tokenized labels of the training set have shape: {train_label_seq.shape}\\n\")\nprint(f\"Tokenized labels of the validation set have shape: {val_label_seq.shape}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:07:23.830198Z","iopub.execute_input":"2023-05-26T10:07:23.830522Z","iopub.status.idle":"2023-05-26T10:07:25.704169Z","shell.execute_reply.started":"2023-05-26T10:07:23.830495Z","shell.execute_reply":"2023-05-26T10:07:25.703095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model for text classification\n`create_model` function defines the model that will classify each text as being part of a certain category. ","metadata":{}},{"cell_type":"code","source":"def create_model(num_words, embedding_dim, maxlen):\n    \"\"\"\n    Creates a text classifier model\n    \n    Args:\n        num_words (int): size of the vocabulary for the Embedding layer input\n        embedding_dim (int): dimensionality of the Embedding layer output\n        maxlen (int): length of the input sequences\n    \n    Returns:\n        model (tf.keras Model): the text classifier model\n    \"\"\"\n    \n    tf.random.set_seed(123)\n    \n    model = tf.keras.Sequential([ \n        tf.keras.layers.Embedding(num_words, embedding_dim, input_length=maxlen),\n        tf.keras.layers.GlobalAveragePooling1D(),\n        tf.keras.layers.Dense(24, activation='relu'),\n        tf.keras.layers.Dense(4, activation='softmax')\n    ])\n    \n    model.compile(loss='sparse_categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:07:25.705733Z","iopub.execute_input":"2023-05-26T10:07:25.706052Z","iopub.status.idle":"2023-05-26T10:07:25.713604Z","shell.execute_reply.started":"2023-05-26T10:07:25.706025Z","shell.execute_reply":"2023-05-26T10:07:25.712500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating a Callback class","metadata":{}},{"cell_type":"code","source":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', \n    factor=0.25,   \n    patience=2, \n    min_lr=0.00001,\n    verbose=2\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:07:25.715113Z","iopub.execute_input":"2023-05-26T10:07:25.715575Z","iopub.status.idle":"2023-05-26T10:07:25.730779Z","shell.execute_reply.started":"2023-05-26T10:07:25.715545Z","shell.execute_reply":"2023-05-26T10:07:25.729259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_filepath = '/kaggle/working/checkpoint'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:07:25.732164Z","iopub.execute_input":"2023-05-26T10:07:25.732567Z","iopub.status.idle":"2023-05-26T10:07:25.742669Z","shell.execute_reply.started":"2023-05-26T10:07:25.732532Z","shell.execute_reply":"2023-05-26T10:07:25.741672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the model","metadata":{}},{"cell_type":"code","source":"model = create_model(NUM_WORDS, EMBEDDING_DIM, MAXLEN)\n\nhistory = model.fit(train_padded_seq, train_label_seq, epochs=30, validation_data=(val_padded_seq, val_label_seq),\n                   callbacks=[model_checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:07:25.743951Z","iopub.execute_input":"2023-05-26T10:07:25.744259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluating Accuracy and Loss for the Model","metadata":{}},{"cell_type":"code","source":"def plot_graphs(history, metric):\n    plt.plot(history.history[metric])\n    plt.plot(history.history[f'val_{metric}'])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(metric)\n    plt.legend([metric, f'val_{metric}'])\n    plt.show()\n    \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluating Precision, Recall, F1-Score and Support for the Model","metadata":{}},{"cell_type":"code","source":"predictions = np.argmax(model.predict(val_padded_seq), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = accuracy_score(predictions, val_label_seq)\nprecision= precision_score(predictions, val_label_seq, average='weighted')\nrecall = recall_score(predictions, val_label_seq, average='weighted')\nf1_score = f1_score(predictions, val_label_seq, average='weighted')\n\nprint(\"Accuracy of the model is {}\".format(round(accuracy,2)))\nprint(\"Precision score  of the model is {}\".format(round(precision,2)))\nprint(\"Recall score of the model is {}\".format(round(recall,2)))\nprint(\"F1-score of the model is {}\".format(round(f1_score,2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_label = ['Household', 'Books', 'Electronics', 'ClothingAccessories']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(predictions, val_label_seq, target_names=class_label))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting the Confusion Matrix for the Classification","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(val_label_seq,predictions)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_label)\nfig, ax = plt.subplots(figsize=(9,9))\ndisp.plot(ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}